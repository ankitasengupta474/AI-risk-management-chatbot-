{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11195174,"sourceType":"datasetVersion","datasetId":6989247},{"sourceId":11206278,"sourceType":"datasetVersion","datasetId":6997159}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:19:49.947517Z","iopub.execute_input":"2025-04-10T10:19:49.947752Z","iopub.status.idle":"2025-04-10T10:19:50.349668Z","shell.execute_reply.started":"2025-04-10T10:19:49.947729Z","shell.execute_reply":"2025-04-10T10:19:50.348452Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\n\nimport joblib","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:20:31.672130Z","iopub.execute_input":"2025-04-10T10:20:31.672485Z","iopub.status.idle":"2025-04-10T10:20:32.688412Z","shell.execute_reply.started":"2025-04-10T10:20:31.672458Z","shell.execute_reply":"2025-04-10T10:20:32.687292Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"file_path=\"/kaggle/input/project-risk-management/it_project_risk_management_dataset (1).csv\"\n\ndf = pd.read_csv(file_path)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:20:37.256645Z","iopub.execute_input":"2025-04-10T10:20:37.257185Z","iopub.status.idle":"2025-04-10T10:20:37.288114Z","shell.execute_reply.started":"2025-04-10T10:20:37.257125Z","shell.execute_reply":"2025-04-10T10:20:37.287120Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"features = [\n    \"total_budget\", \"current_spend\", \"resource_utilization\", \"delay_days\", \"budget_variance\",\n    \"customer_payments_received\", \"tech_sector_performance\", \"market_volatility_index\",\n    \"company_stock_performance\", \"technology_complexity\", \"integration_challenges\",\n    \"team_turnover_rate\", \"regulatory_changes\", \"data_privacy_compliance\"\n]\ntarget = \"overall_risk_score\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:20:40.861030Z","iopub.execute_input":"2025-04-10T10:20:40.861417Z","iopub.status.idle":"2025-04-10T10:20:40.865602Z","shell.execute_reply.started":"2025-04-10T10:20:40.861375Z","shell.execute_reply":"2025-04-10T10:20:40.864666Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = df[features]\ny = df[target]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standardizing the data\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:20:43.045866Z","iopub.execute_input":"2025-04-10T10:20:43.046235Z","iopub.status.idle":"2025-04-10T10:20:43.077984Z","shell.execute_reply.started":"2025-04-10T10:20:43.046205Z","shell.execute_reply":"2025-04-10T10:20:43.076823Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"param_grid = {\n    'n_estimators': [100, 200, 300, 500],\n    'max_depth': [10, 20, 30, None],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n    'bootstrap': [True, False]\n}\n\n# Initializing the model\nrf = RandomForestRegressor(random_state=42)\n\n# Randomized search with cross-validation\nrandom_search = RandomizedSearchCV(\n    estimator=rf, param_distributions=param_grid,\n    n_iter=20, cv=5, verbose=1, n_jobs=-1, random_state=42\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:20:45.222714Z","iopub.execute_input":"2025-04-10T10:20:45.223111Z","iopub.status.idle":"2025-04-10T10:20:45.228901Z","shell.execute_reply.started":"2025-04-10T10:20:45.223083Z","shell.execute_reply":"2025-04-10T10:20:45.227649Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fitting the model\nrandom_search.fit(X_train_scaled, y_train)\n\n# Best model\nbest_model = random_search.best_estimator_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:20:47.329470Z","iopub.execute_input":"2025-04-10T10:20:47.329845Z","iopub.status.idle":"2025-04-10T10:21:51.947370Z","shell.execute_reply.started":"2025-04-10T10:20:47.329816Z","shell.execute_reply":"2025-04-10T10:21:51.945884Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, r2_score\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error, r2_score\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:22:05.817229Z","iopub.execute_input":"2025-04-10T10:22:05.817599Z","iopub.status.idle":"2025-04-10T10:22:05.822645Z","shell.execute_reply.started":"2025-04-10T10:22:05.817569Z","shell.execute_reply":"2025-04-10T10:22:05.821265Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = best_model.predict(X_test_scaled)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"Model RMSE: {rmse}\")\nprint(f\"Model R² Score: {r2}\")\n\n# Saving the optimized model and scaler in Kaggle output directory\noptimized_model_path = \"/kaggle/working/optimized_risk_prediction_model.pkl\"\nscaler_path = \"/kaggle/working/scaler.pkl\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:22:07.922225Z","iopub.execute_input":"2025-04-10T10:22:07.922577Z","iopub.status.idle":"2025-04-10T10:22:07.942032Z","shell.execute_reply.started":"2025-04-10T10:22:07.922548Z","shell.execute_reply":"2025-04-10T10:22:07.941033Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"joblib.dump(best_model, optimized_model_path)\njoblib.dump(scaler, scaler_path)\n\nprint(f\"Optimized model saved at: {optimized_model_path}\")\nprint(f\"Scaler saved at: {scaler_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:22:12.337780Z","iopub.execute_input":"2025-04-10T10:22:12.338112Z","iopub.status.idle":"2025-04-10T10:22:12.418719Z","shell.execute_reply.started":"2025-04-10T10:22:12.338088Z","shell.execute_reply":"2025-04-10T10:22:12.417882Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import joblib\n\ndef load_model():\n    return joblib.load(\"/kaggle/working/optimized_risk_prediction_model.pkl\")\n\nmodel = load_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:22:14.150435Z","iopub.execute_input":"2025-04-10T10:22:14.150748Z","iopub.status.idle":"2025-04-10T10:22:14.215308Z","shell.execute_reply.started":"2025-04-10T10:22:14.150724Z","shell.execute_reply":"2025-04-10T10:22:14.214347Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install chromadb sentence-transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:22:16.564161Z","iopub.execute_input":"2025-04-10T10:22:16.564481Z","iopub.status.idle":"2025-04-10T10:22:52.134386Z","shell.execute_reply.started":"2025-04-10T10:22:16.564456Z","shell.execute_reply":"2025-04-10T10:22:52.133290Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install -U langchain-huggingface","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:27:02.585053Z","iopub.execute_input":"2025-04-10T10:27:02.585531Z","iopub.status.idle":"2025-04-10T10:27:06.995256Z","shell.execute_reply.started":"2025-04-10T10:27:02.585500Z","shell.execute_reply":"2025-04-10T10:27:06.994197Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import chromadb\nfrom sentence_transformers import SentenceTransformer\nimport pandas as pd\n\n# Initialize ChromaDB Persistent Storage\nchroma_client = chromadb.PersistentClient(path=\"./chromadb_store\")\n\n# Load a local embedding model (no API key required)\nembedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\n# Create a ChromaDB collection\ncollection = chroma_client.get_or_create_collection(name=\"project_risks\")\n\n# Create project descriptions based on the existing columns\ndf['project_description'] = df.apply(\n    lambda row: f\"{row['project_name']} in the {row['industry']} industry, managed by {row['company']}.\", axis=1\n)\n\n# Convert project descriptions into vector embeddings\ndf[\"embeddings\"] = df[\"project_description\"].apply(lambda x: embedding_model.encode(x).tolist())\nfor idx, row in df.iterrows():\n    collection.add(\n        ids=[str(idx)],  # IDs must be strings\n        embeddings=[row[\"embeddings\"]],  # List of embeddings (each embedding must be a list)\n        metadatas=[{\n            \"project_id\": row[\"project_id\"],\n            \"risk_score\": row[\"overall_risk_score\"]\n        }]\n    )\n\nprint(\"Embeddings stored successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:27:17.837680Z","iopub.execute_input":"2025-04-10T10:27:17.838075Z","iopub.status.idle":"2025-04-10T10:29:00.376483Z","shell.execute_reply.started":"2025-04-10T10:27:17.838039Z","shell.execute_reply":"2025-04-10T10:29:00.375271Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nrf_model = RandomForestRegressor()  # Machine learning model\nembedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # NLP model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:29:04.534993Z","iopub.execute_input":"2025-04-10T10:29:04.535995Z","iopub.status.idle":"2025-04-10T10:29:05.801953Z","shell.execute_reply.started":"2025-04-10T10:29:04.535958Z","shell.execute_reply":"2025-04-10T10:29:05.800880Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def query_risk_analysis(query_text, top_k=3):\n    query_embedding = embedding_model.encode(query_text).tolist()  # Use correct model\n    results = collection.query(query_embeddings=[query_embedding], n_results=top_k)\n    print(\"\\nTop Risk Factors:\", results)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:29:09.636852Z","iopub.execute_input":"2025-04-10T10:29:09.637251Z","iopub.status.idle":"2025-04-10T10:29:09.642336Z","shell.execute_reply.started":"2025-04-10T10:29:09.637218Z","shell.execute_reply":"2025-04-10T10:29:09.641232Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def query_risk_analysis(query_text, top_k=3):\n    \"\"\"\n    Given a project description or risk-related query, \n    this function retrieves similar risks and calculates an overall risk score.\n    \"\"\"\n    # Generate embedding for the query text\n    query_embedding = embedding_model.encode(query_text).tolist()\n\n    # Query the vector database for relevant risk factors\n    results = collection.query(query_embeddings=[query_embedding], n_results=top_k)\n\n    # Extract risk scores (assuming results contain risk severity scores)\n    if \"documents\" in results and results[\"documents\"]:\n        risk_factors = results[\"documents\"][0]  # Extract the top results\n\n        # Extract risk scores if available\n        risk_scores = [item.get(\"score\", 0) for item in risk_factors]  \n\n        # Calculate the average risk score\n        avg_risk = np.mean(risk_scores) if risk_scores else 0  \n        \n        # Define risk categories based on score\n        if avg_risk >= 0.7:\n            risk_level = \"High\"\n        elif avg_risk >= 0.4:\n            risk_level = \"Medium\"\n        else:\n            risk_level = \"Low\"\n\n        # Print the results\n        print(f\"\\nTop Risk Factors: {risk_factors}\")\n        print(f\"Overall Risk Score: {avg_risk:.2f} ({risk_level})\")\n\n        return {\n            \"risk_factors\": risk_factors,\n            \"overall_risk\": avg_risk,\n            \"risk_level\": risk_level\n        }\n    else:\n        print(\"\\nNo relevant risk factors found in the database.\")\n        return {\"risk_factors\": [], \"overall_risk\": 0, \"risk_level\": \"Unknown\"}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:29:12.961705Z","iopub.execute_input":"2025-04-10T10:29:12.962133Z","iopub.status.idle":"2025-04-10T10:29:12.969418Z","shell.execute_reply.started":"2025-04-10T10:29:12.962098Z","shell.execute_reply":"2025-04-10T10:29:12.968287Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def query_risk_analysis(query_text, top_k=3):\n    \"\"\"\n    Given a project description or risk-related query, \n    this function retrieves similar risks and calculates an overall risk score.\n    \"\"\"\n    # Generate embedding for the query text\n    query_embedding = embedding_model.encode(query_text).tolist()\n\n    # Query the vector database for relevant risk factors\n    results = collection.query(query_embeddings=[query_embedding], n_results=top_k)\n\n    # Extract risk scores (assuming results contain risk severity scores)\n    if \"documents\" in results and results[\"documents\"]:\n        risk_factors = results[\"documents\"][0]  # Extract the top results\n\n        # Extract risk scores if available\n        risk_scores = [item.get(\"score\", 0) for item in risk_factors]  \n\n        # Calculate the average risk score\n        avg_risk = np.mean(risk_scores) if risk_scores else 0  \n        \n        # Define risk categories based on score\n        if avg_risk >= 0.7:\n            risk_level = \"High\"\n        elif avg_risk >= 0.4:\n            risk_level = \"Medium\"\n        else:\n            risk_level = \"Low\"\n\n        # Print the results\n        print(f\"\\nTop Risk Factors: {risk_factors}\")\n        print(f\"Overall Risk Score: {avg_risk:.2f} ({risk_level})\")\n\n        return {\n            \"risk_factors\": risk_factors,\n            \"overall_risk\": avg_risk,\n            \"risk_level\": risk_level\n        }\n    else:\n        print(\"\\nNo relevant risk factors found in the database.\")\n        return {\"risk_factors\": [], \"overall_risk\": 0, \"risk_level\": \"Unknown\"}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:29:15.858647Z","iopub.execute_input":"2025-04-10T10:29:15.859001Z","iopub.status.idle":"2025-04-10T10:29:15.866160Z","shell.execute_reply.started":"2025-04-10T10:29:15.858973Z","shell.execute_reply":"2025-04-10T10:29:15.865091Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import necessary libraries(REAL)\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\nimport chromadb  # Use ChromaDB or Pinecone based on your setup\n\n# Load a pre-trained NLP model for text embeddings\nembedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # You can change the model\n\n# Initialize the vector database (ChromaDB as an example)\nclient = chromadb.PersistentClient(path=\"risk_analysis_db\")  # Ensure path exists\ncollection = client.get_or_create_collection(name=\"project_risks\")\n\ndef query_risk_analysis(query_text, top_k=3):\n    \"\"\"\n    Given a project description or risk-related query, \n    this function retrieves similar risks and calculates an overall risk score.\n    \"\"\"\n    # Generate embedding for the query text\n    query_embedding = embedding_model.encode(query_text).tolist()\n\n    # Query the vector database for relevant risk factors\n    results = collection.query(query_embeddings=[query_embedding], n_results=top_k)\n\n    # Extract risk scores (assuming results contain risk severity scores)\n    if \"documents\" in results and results[\"documents\"]:\n        risk_factors = results[\"documents\"][0]  # Extract the top results\n\n        # Extract risk scores if available\n        risk_scores = [item.get(\"score\", 0) for item in risk_factors]  \n\n        # Calculate the average risk score\n        avg_risk = np.mean(risk_scores) if risk_scores else 0  \n        \n        # Define risk categories based on score\n        if avg_risk >= 0.7:\n            risk_level = \"High\"\n        elif avg_risk >= 0.4:\n            risk_level = \"Medium\"\n        else:\n            risk_level = \"Low\"\n\n        # Print the results\n        print(f\"\\nTop Risk Factors: {risk_factors}\")\n        print(f\"Overall Risk Score: {avg_risk:.2f} ({risk_level})\")\n\n        return {\n            \"risk_factors\": risk_factors,\n            \"overall_risk\": avg_risk,\n            \"risk_level\": risk_level\n        }\n    else:\n        print(\"\\nNo relevant risk factors found in the database.\")\n        return {\"risk_factors\": [], \"overall_risk\": 0, \"risk_level\": \"Unknown\"}\n\n# Example Query\nquery_text = \"What are the risks for an AI-based cloud computing project?\"\nquery_risk_analysis(query_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:29:18.733587Z","iopub.execute_input":"2025-04-10T10:29:18.734023Z","iopub.status.idle":"2025-04-10T10:29:19.996768Z","shell.execute_reply.started":"2025-04-10T10:29:18.733988Z","shell.execute_reply":"2025-04-10T10:29:19.995740Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nrf_model = RandomForestRegressor()  # Machine learning model\nembedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # NLP model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:29:22.363173Z","iopub.execute_input":"2025-04-10T10:29:22.363509Z","iopub.status.idle":"2025-04-10T10:29:23.449193Z","shell.execute_reply.started":"2025-04-10T10:29:22.363483Z","shell.execute_reply":"2025-04-10T10:29:23.448084Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def query_risk_analysis(query_text, top_k=3):\n    query_embedding = embedding_model.encode(query_text).tolist()\n    \n    # Retrieve results\n    results = collection.query(query_embeddings=[query_embedding], n_results=top_k)\n    \n    # DEBUG: Print raw results\n    print(\"\\nRaw Query Results:\", results)\n    \n    # Extract relevant documents\n    if \"documents\" in results and results[\"documents\"]:\n        risk_factors = results[\"documents\"][0]\n        print(\"\\nExtracted Risk Factors:\", risk_factors)\n\n        risk_scores = [item.get(\"score\", 0) for item in risk_factors]  \n        avg_risk = np.mean(risk_scores) if risk_scores else 0  \n\n        if avg_risk >= 0.7:\n            risk_level = \"High\"\n        elif avg_risk >= 0.4:\n            risk_level = \"Medium\"\n        else:\n            risk_level = \"Low\"\n\n        print(f\"\\nOverall Risk Score: {avg_risk:.2f} ({risk_level})\")\n        return {\"risk_factors\": risk_factors, \"overall_risk\": avg_risk, \"risk_level\": risk_level}\n    \n    print(\"\\nNo relevant risk factors found in the database.\")\n    return {\"risk_factors\": [], \"overall_risk\": 0, \"risk_level\": \"Low\"}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:29:28.437958Z","iopub.execute_input":"2025-04-10T10:29:28.438350Z","iopub.status.idle":"2025-04-10T10:29:28.445250Z","shell.execute_reply.started":"2025-04-10T10:29:28.438320Z","shell.execute_reply":"2025-04-10T10:29:28.444245Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"query_text = \"What are the risks for an AI-based healthcare project?\"\nquery_risk_analysis(query_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:29:32.064316Z","iopub.execute_input":"2025-04-10T10:29:32.064708Z","iopub.status.idle":"2025-04-10T10:29:32.120056Z","shell.execute_reply.started":"2025-04-10T10:29:32.064674Z","shell.execute_reply":"2025-04-10T10:29:32.119231Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\nimport chromadb\nfrom sentence_transformers import SentenceTransformer\n\n# Initialize ChromaDB Persistent Storage\nchroma_client = chromadb.PersistentClient(path=\"./chromadb_store\")\n\n# Load a local embedding model (no API key required)\nembedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\n# Create a ChromaDB collection\ncollection = chroma_client.get_or_create_collection(name=\"project_risks\")\n\n# Convert project descriptions into vector embeddings\ndf[\"embeddings\"] = df[\"project_description\"].apply(lambda x: embedding_model.encode(x).tolist())\n\n# Store embeddings in ChromaDB\nfor idx, row in df.iterrows():\n    collection.add(\n        ids=[str(idx)],\n        embeddings=[row[\"embeddings\"]],\n        metadatas=[{\"project_id\": row[\"project_id\"], \"risk_score\": row[\"overall_risk_score\"]}]\n    )\n\nprint(\"Embeddings stored successfully!\")\n\n# Perform similarity search\nquery_text = \"High budget variance and technology complexity\"\nquery_embedding = embedding_model.encode(query_text).tolist()\n\nresults = collection.query(\n    query_embeddings=[query_embedding],\n    n_results=5\n)\n\nprint(\"Top similar projects:\", results)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:29:34.420639Z","iopub.execute_input":"2025-04-10T10:29:34.420991Z","iopub.status.idle":"2025-04-10T10:30:44.372398Z","shell.execute_reply.started":"2025-04-10T10:29:34.420962Z","shell.execute_reply":"2025-04-10T10:30:44.371347Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\ndef query_risk_analysis(query_text, top_k=5):\n    # Generate query embedding\n    query_embedding = embedding_model.encode(query_text).tolist()\n    \n    # Retrieve similar projects from ChromaDB\n    results = collection.query(\n        query_embeddings=[query_embedding],\n        n_results=top_k\n    )\n    \n    # DEBUG: Print raw results\n    print(\"\\nRaw Query Results:\", results)\n\n    # Extract risk scores from retrieved projects\n    risk_scores = []\n    if \"metadatas\" in results and results[\"metadatas\"]:\n        for metadata in results[\"metadatas\"][0]:  \n            risk_scores.append(metadata.get(\"risk_score\", 0))  \n\n    # If no risk scores are found, return Low risk\n    if not risk_scores:\n        print(\"\\nNo relevant risk factors found.\")\n        return {\"risk_factors\": [], \"overall_risk\": 0, \"risk_level\": \"Low\"}\n    \n    # Calculate the overall risk score\n    avg_risk = np.mean(risk_scores)\n    \n    # Assign risk level\n    if avg_risk >= 0.7:\n        risk_level = \"High\"\n    elif avg_risk >= 0.4:\n        risk_level = \"Medium\"\n    else:\n        risk_level = \"Low\"\n\n    print(f\"\\nTop Risk Factors: {risk_scores}\")\n    print(f\"Overall Risk Score: {avg_risk:.2f} ({risk_level})\")\n\n    return {\"risk_factors\": risk_scores, \"overall_risk\": avg_risk, \"risk_level\": risk_level}\n\n# Example Query\nquery_text = \"High budget variance and technology complexity\"\nquery_risk_analysis(query_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:31:04.968825Z","iopub.execute_input":"2025-04-10T10:31:04.969270Z","iopub.status.idle":"2025-04-10T10:31:05.015947Z","shell.execute_reply.started":"2025-04-10T10:31:04.969236Z","shell.execute_reply":"2025-04-10T10:31:05.014903Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\ndef query_risk_analysis(query_text, top_k=5):\n    query_embedding = embedding_model.encode(query_text).tolist()\n    \n    results = collection.query(\n        query_embeddings=[query_embedding],\n        n_results=top_k\n    )\n\n    risk_scores = [\n        metadata.get(\"risk_score\", 0) \n        for metadata in results.get(\"metadatas\", [[]])[0] \n        if metadata\n    ]\n\n    if not risk_scores:\n        return {\"risk_factors\": [], \"overall_risk\": 0, \"risk_level\": \"Low\"}\n    \n    avg_risk = np.mean(risk_scores)\n\n    risk_level = \"High\" if avg_risk >= 0.7 else \"Medium\" if avg_risk >= 0.4 else \"Low\"\n\n    return {\"risk_factors\": risk_scores, \"overall_risk\": avg_risk, \"risk_level\": risk_level}\n\n# Example Query\nquery_text = \"High budget variance and technology complexity\"\nresult = query_risk_analysis(query_text)\nprint(result)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:31:08.405754Z","iopub.execute_input":"2025-04-10T10:31:08.406110Z","iopub.status.idle":"2025-04-10T10:31:08.450478Z","shell.execute_reply.started":"2025-04-10T10:31:08.406070Z","shell.execute_reply":"2025-04-10T10:31:08.449499Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torch\n!pip install fastapi\n!pip install streamlit\n!pip install uvicorn\n!pip install requests","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:31:11.831618Z","iopub.execute_input":"2025-04-10T10:31:11.832019Z","iopub.status.idle":"2025-04-10T10:31:34.673157Z","shell.execute_reply.started":"2025-04-10T10:31:11.831991Z","shell.execute_reply":"2025-04-10T10:31:34.671804Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install transformers\n!pip install torch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:31:52.273371Z","iopub.execute_input":"2025-04-10T10:31:52.273729Z","iopub.status.idle":"2025-04-10T10:32:00.767130Z","shell.execute_reply.started":"2025-04-10T10:31:52.273700Z","shell.execute_reply":"2025-04-10T10:32:00.765909Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n!pip install transformers\n!pip install datasets\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:32:05.043608Z","iopub.execute_input":"2025-04-10T10:32:05.043969Z","iopub.status.idle":"2025-04-10T10:32:13.589533Z","shell.execute_reply.started":"2025-04-10T10:32:05.043940Z","shell.execute_reply":"2025-04-10T10:32:13.588232Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install groq\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:32:26.033023Z","iopub.execute_input":"2025-04-10T10:32:26.033484Z","iopub.status.idle":"2025-04-10T10:32:30.567733Z","shell.execute_reply.started":"2025-04-10T10:32:26.033442Z","shell.execute_reply":"2025-04-10T10:32:30.566571Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"GROQ_API\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:33:10.201055Z","iopub.execute_input":"2025-04-10T10:33:10.201508Z","iopub.status.idle":"2025-04-10T10:33:10.420304Z","shell.execute_reply.started":"2025-04-10T10:33:10.201477Z","shell.execute_reply":"2025-04-10T10:33:10.419035Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install transformers torch chromadb sentence-transformers accelerate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:33:21.654945Z","iopub.execute_input":"2025-04-10T10:33:21.655339Z","iopub.status.idle":"2025-04-10T10:33:26.437718Z","shell.execute_reply.started":"2025-04-10T10:33:21.655307Z","shell.execute_reply":"2025-04-10T10:33:26.436316Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Load Groq API Key from Kaggle Secrets and inject into environment ---\n\nfrom kaggle_secrets import UserSecretsClient\nimport os\n\nuser_secrets = UserSecretsClient()\nsecret_value = user_secrets.get_secret(\"GROQ_API\")\nos.environ[\"GROQ_API_KEY\"] = secret_value\n\nprint(\"✅ Groq API Key loaded successfully from Kaggle Secrets.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:33:39.330057Z","iopub.execute_input":"2025-04-10T10:33:39.330514Z","iopub.status.idle":"2025-04-10T10:33:39.522192Z","shell.execute_reply.started":"2025-04-10T10:33:39.330480Z","shell.execute_reply":"2025-04-10T10:33:39.521200Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import datetime\nimport logging\nimport numpy as np\nimport chromadb\nfrom groq import Groq\nfrom sentence_transformers import SentenceTransformer\nimport matplotlib.pyplot as plt\nimport io\nimport base64\nfrom typing import List, Dict, Any\nimport os  # Added for environment variable option\n\n# Check if tkinter is available (for GUI mode)\nHAS_TKINTER = False\ntry:\n    import tkinter as tk\n    from tkinter import ttk, scrolledtext, font\n    from matplotlib.figure import Figure\n    from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n    # Test if display is available\n    root = tk.Tk()\n    root.destroy()\n    HAS_TKINTER = True\nexcept Exception:\n    # Running in environment without display (like Kaggle)\n    HAS_TKINTER = False\n\nclass ProjectKnowledgeBase:\n    \"\"\"Contains information about project challenges and mitigation strategies.\"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize with project-specific information.\"\"\"\n        self.project_challenges = {\n            \"budget\": {\n                \"description\": \"The project is experiencing budget overruns due to unexpected material cost increases and extended timelines.\",\n                \"mitigation\": \"Implement stricter cost control measures, negotiate with vendors for better rates, and review the project scope to identify non-essential features that could be deprioritized.\"\n            },\n            \"timeline\": {\n                \"description\": \"The project is behind schedule due to resource constraints and technical integration challenges.\",\n                \"mitigation\": \"Revise the project schedule, add additional experienced resources to critical path tasks, and implement daily stand-up meetings to quickly address blockers.\"\n            },\n            \"resources\": {\n                \"description\": \"Key team members have limited availability due to competing priorities and unexpected turnover.\",\n                \"mitigation\": \"Cross-train team members, document knowledge to reduce dependency on specific individuals, and establish clear escalation paths for resource conflicts.\"\n            },\n            \"technical\": {\n                \"description\": \"Integration with legacy systems is proving more complex than anticipated, causing delays and quality issues.\",\n                \"mitigation\": \"Bring in technical specialists familiar with the legacy systems, create a dedicated integration testing environment, and implement detailed error logging for faster troubleshooting.\"\n            },\n            \"quality\": {\n                \"description\": \"User acceptance testing has identified several critical defects that require significant rework.\",\n                \"mitigation\": \"Implement more rigorous code reviews, add automated testing for regression issues, and conduct earlier stakeholder reviews to identify problems sooner.\"\n            },\n            \"stakeholder\": {\n                \"description\": \"Stakeholders have changing requirements and expectations, creating scope creep and confusion.\",\n                \"mitigation\": \"Implement formal change management processes, conduct regular stakeholder alignment sessions, and maintain a prioritized backlog of requirements.\"\n            },\n            \"market\": {\n                \"description\": \"Market conditions are shifting, potentially impacting the project's business case and priorities.\",\n                \"mitigation\": \"Conduct regular market analysis, maintain flexibility in the implementation approach, and keep the business case updated to reflect changing conditions.\"\n            }\n        }\n        \n        # General project information\n        self.project_info = {\n            \"name\": \"Enterprise Digital Transformation Initiative\",\n            \"objective\": \"Modernize legacy systems and implement new digital capabilities to improve operational efficiency and customer experience.\",\n            \"timeline\": \"12 months (Currently in month 7)\",\n            \"budget\": \"$2.8M\",\n            \"team_size\": \"15 core team members plus extended stakeholders\",\n            \"key_milestones\": [\n                \"Requirements & Design: Months 1-3 (Completed)\",\n                \"Development Phase 1: Months 3-6 (Completed)\",\n                \"Development Phase 2: Months 6-9 (In Progress - Currently Delayed)\",\n                \"Testing & Integration: Months 8-11 (Not Started)\",\n                \"Deployment & Handover: Months 11-12 (Not Started)\"\n            ]\n        }\n    \n    def get_project_info(self):\n        \"\"\"Returns general project information.\"\"\"\n        return self.project_info\n    \n    def get_challenge_info(self, challenge_type):\n        \"\"\"Returns information about a specific challenge type.\"\"\"\n        challenge_type = challenge_type.lower()\n        for key, info in self.project_challenges.items():\n            if challenge_type in key:\n                return {key: info}\n        return None\n    \n    def get_all_challenges(self):\n        \"\"\"Returns all project challenges.\"\"\"\n        return self.project_challenges\n    \n    def search_challenges(self, query):\n        \"\"\"Searches for challenges related to the query.\"\"\"\n        query = query.lower()\n        relevant_challenges = {}\n        \n        # Map common terms to challenge categories\n        keyword_mapping = {\n            \"cost\": \"budget\", \"expense\": \"budget\", \"money\": \"budget\", \"finance\": \"budget\",\n            \"schedule\": \"timeline\", \"deadline\": \"timeline\", \"late\": \"timeline\", \"delay\": \"timeline\",\n            \"staff\": \"resources\", \"team\": \"resources\", \"personnel\": \"resources\", \"hiring\": \"resources\",\n            \"system\": \"technical\", \"integration\": \"technical\", \"code\": \"technical\", \"architecture\": \"technical\",\n            \"defect\": \"quality\", \"bug\": \"quality\", \"testing\": \"quality\", \"standards\": \"quality\",\n            \"client\": \"stakeholder\", \"management\": \"stakeholder\", \"scope\": \"stakeholder\",\n            \"competition\": \"market\", \"industry\": \"market\", \"economic\": \"market\"\n        }\n        \n        # Check for direct challenge types first\n        for challenge in self.project_challenges.keys():\n            if challenge in query:\n                relevant_challenges[challenge] = self.project_challenges[challenge]\n        \n        # If no direct matches, check for related terms\n        if not relevant_challenges:\n            for keyword, challenge in keyword_mapping.items():\n                if keyword in query and challenge not in relevant_challenges:\n                    relevant_challenges[challenge] = self.project_challenges[challenge]\n        \n        return relevant_challenges if relevant_challenges else self.project_challenges\n\nclass ProjectRiskAnalysisChatbot:\n    def __init__(\n        self, \n        groq_api_key=None,  # You can provide your API key here when initializing\n        model_name=\"llama3-8b-8192\", \n        embedding_model=\"all-MiniLM-L6-v2\",\n        use_gui=None  # None means auto-detect\n    ):\n        \"\"\"\n        Initialize the Risk Analysis Chatbot using Groq AI\n        \n        Args:\n            groq_api_key (str): Your Groq API key\n            model_name (str): Groq model identifier\n            embedding_model (str): Sentence Transformer for embeddings\n            use_gui (bool): Whether to use GUI. If None, auto-detects.\n        \"\"\"\n        # Configure logging\n        logging.basicConfig(level=logging.INFO)\n        self.logger = logging.getLogger(__name__)\n\n        # Determine if GUI should be used\n        if use_gui is None:\n            self.use_gui = HAS_TKINTER\n        else:\n            self.use_gui = use_gui and HAS_TKINTER\n            \n        if self.use_gui:\n            self.logger.info(\"Running in GUI mode\")\n        else:\n            self.logger.info(\"Running in CLI mode\")\n\n        try:\n            # Initialize ChromaDB for vector storage\n            self.chroma_client = chromadb.PersistentClient(path=\"./risk_analysis_memory\")\n            self.collection = self.chroma_client.get_or_create_collection(\n                name=\"project_risks\"\n            )\n\n            # Load embedding model\n            self.embedding_model = SentenceTransformer(embedding_model)\n\n            # Initialize Groq client\n            self.groq_client = Groq(api_key=groq_api_key)\n            self.model_name = model_name\n\n            # Initialize risk analysis agents\n            self.market_agent = MarketAnalysisAgent()\n            self.scoring_agent = RiskScoringAgent(self.collection, self.embedding_model)\n            self.tracking_agent = ProjectStatusTrackingAgent()\n            self.reporting_agent = ReportingAgent()\n            \n            # Add knowledge base\n            self.knowledge_base = ProjectKnowledgeBase()\n            \n            # Initialize UI\n            self.root = None\n            self.recent_report = None\n\n        except Exception as e:\n            self.logger.error(f\"Initialization error: {e}\")\n            raise\n    \n    def is_risk_related(self, query_text):\n        \"\"\"\n        Check if a query is related to project risks, challenges, or management\n        \n        Args:\n            query_text (str): User's query\n            \n        Returns:\n            bool: True if query is related to project information, False otherwise\n        \"\"\"\n        project_keywords = [\n            # Original risk keywords\n            \"risk\", \"project\", \"budget\", \"cost\", \"deadline\", \"delay\", \"timeline\",\n            \"resource\", \"team\", \"technical\", \"quality\", \"market\", \"financial\",\n            \"competitor\", \"competition\", \"challenge\", \"issue\", \"problem\", \"concern\",\n            \"schedule\", \"staff\", \"technology\", \"system\", \"requirement\",\n            \n            # Added more general project keywords\n            \"task\", \"objective\", \"goal\", \"milestone\", \"deliverable\", \"status\",\n            \"progress\", \"mitigation\", \"strategy\", \"solve\", \"solution\", \"fix\",\n            \"handle\", \"address\", \"manage\", \"approach\", \"stakeholder\", \"sponsor\",\n            \"scope\", \"feature\", \"implementation\", \"plan\", \"phase\", \"testing\",\n            \"deployment\", \"release\", \"development\", \"design\", \"integration\",\n            \"what\", \"how\", \"why\", \"when\", \"who\"  # General question words\n        ]\n        \n        query_lower = query_text.lower()\n        return \"challenge\" in query_lower or any(keyword in query_lower for keyword in project_keywords)\n\n\n    def analyze_project_risks(self, query_text):\n        \"\"\"\n        Comprehensive project risk analysis\n        \n        Args:\n            query_text (str): User's risk query\n        \n        Returns:\n            dict: Detailed risk analysis report\n        \"\"\"\n        try:\n            # Perform multi-agent risk analysis\n            market_risk = self.market_agent.analyze_market_risk(query_text)\n            risk_data = self.scoring_agent.query_risk_analysis(query_text)\n            status_risk = self.tracking_agent.track_project_status()\n\n            # Get risk names and values\n            risk_names = []\n            risk_values = []\n            \n            # Market risks\n            market_names = [\"Market Volatility\", \"Financial Trends\", \"Industry Competition\"]\n            for i, score in enumerate(market_risk.get(\"risk_factors\", [])):\n                if i < len(market_names):\n                    risk_names.append(market_names[i])\n                    risk_values.append(score)\n            \n            # Internal risks\n            internal_names = [\"Budget Issues\", \"Timeline Delays\", \"Resource Constraints\", \n                              \"Technical Challenges\", \"Quality Issues\"]\n            for i, score in enumerate(risk_data.get(\"risk_factors\", [])):\n                if i < len(internal_names):\n                    risk_names.append(internal_names[i])\n                    risk_values.append(score)\n                    \n            # Status risks\n            status_names = [\"Project Delays\", \"Team Member Changes\"]\n            for i, score in enumerate(status_risk.get(\"risk_factors\", [])):\n                if i < len(status_names):\n                    risk_names.append(status_names[i])\n                    risk_values.append(score)\n\n            # Combine all risk factors\n            all_risks = (\n                risk_data.get(\"risk_factors\", []) + \n                market_risk.get(\"risk_factors\", []) + \n                status_risk.get(\"risk_factors\", [])\n            )\n\n            # Calculate overall risk\n            overall_risk = np.mean(all_risks) if all_risks else 0\n            risk_level = (\n                \"High\" if overall_risk >= 0.7 \n                else \"Medium\" if overall_risk >= 0.4 \n                else \"Low\"\n            )\n\n            # Generate detailed report\n            report = {\n                \"query\": query_text,\n                \"risk_factors\": all_risks,\n                \"risk_names\": risk_names,\n                \"risk_values\": risk_values,\n                \"overall_risk\": overall_risk,\n                \"risk_level\": risk_level,\n                \"timestamp\": datetime.datetime.now().isoformat()\n            }\n\n            # Generate AI-enhanced explanation with the query text\n            report['explanation'] = self.generate_risk_explanation(report, query_text)\n\n            # Save the latest report\n            self.recent_report = report\n            \n            return report\n\n        except Exception as e:\n            self.logger.error(f\"Risk analysis error: {e}\")\n            return {\"error\": str(e)}\n\n    def generate_risk_explanation(self, report, query_text=None):\n        \"\"\"\n        Generate natural language explanation of risk report using Groq AI\n        with focus on actual project challenges and mitigation strategies\n        \n        Args:\n            report (dict): Risk analysis report\n            query_text (str): Original user query\n        \n        Returns:\n            str: AI-generated risk explanation\n        \"\"\"\n        try:\n            if query_text is None:\n                query_text = report.get('query', '')\n            \n            # Determine if this is about challenges, mitigations, or general risk\n            query_lower = query_text.lower()\n            is_about_challenges = any(word in query_lower for word in ['challenge', 'problem', 'issue', 'concern'])\n            is_about_mitigation = any(word in query_lower for word in ['mitigate', 'solve', 'address', 'solution', 'fix', 'handle'])\n            \n            # Get relevant challenge information\n            relevant_challenges = self.knowledge_base.search_challenges(query_lower)\n            challenge_info = \"\"\n            for challenge_type, info in relevant_challenges.items():\n                challenge_info += f\"- {challenge_type.title()}: {info['description']}\\n\"\n                if is_about_mitigation:\n                    challenge_info += f\"  Mitigation: {info['mitigation']}\\n\"\n            \n            # Get risk names and scores for the prompt\n            risk_details = \"\"\n            if 'risk_names' in report and 'risk_values' in report:\n                for name, value in zip(report['risk_names'], report['risk_values']):\n                    risk_level = \"HIGH\" if value >= 0.7 else \"MEDIUM\" if value >= 0.4 else \"LOW\"\n                    risk_details += f\"- {name}: {value:.2f} ({risk_level})\\n\"\n            \n            # Get project information\n            project_info = self.knowledge_base.get_project_info()\n            project_summary = f\"Project: {project_info['name']}\\nObjective: {project_info['objective']}\\nTimeline: {project_info['timeline']}\\nBudget: {project_info['budget']}\\n\"\n            \n            # Prepare appropriate prompt based on query type\n            if is_about_challenges and not is_about_mitigation:\n                prompt = f\"\"\"\n                The user asked about project challenges: \"{query_text}\"\n                \n                Based on the following information, provide a detailed explanation of the relevant project challenges:\n                \n                {project_summary}\n                \n                Risk Assessment:\n                - Overall Risk Level: {report['risk_level']} ({report['overall_risk']:.2f})\n                \n                Relevant Project Challenges:\n                {challenge_info}\n                \n                Focus on explaining the challenges in detail. Include specific issues the project is facing and why they matter. \n                Be specific and practical in your explanation.\n                \"\"\"\n            elif is_about_mitigation:\n                prompt = f\"\"\"\n                The user asked about mitigating project challenges: \"{query_text}\"\n                \n                Based on the following information, provide detailed mitigation strategies for the relevant challenges:\n                \n                {project_summary}\n                \n                Risk Assessment:\n                - Overall Risk Level: {report['risk_level']} ({report['overall_risk']:.2f})\n                \n                Relevant Project Challenges and Mitigation Strategies:\n                {challenge_info}\n                \n                Focus on explaining the mitigation strategies in detail. Include specific actions that can be taken to address each challenge.\n                Provide practical and actionable advice.\n                \"\"\"\n            else:\n                prompt = f\"\"\"\n                The user asked: \"{query_text}\"\n                \n                Provide a business-focused explanation of the following project and its risks:\n                \n                {project_summary}\n                \n                Risk Assessment:\n                - Overall Risk Level: {report['risk_level']} ({report['overall_risk']:.2f})\n                - Key Risk Factors:\n                {risk_details}\n                \n                Relevant Project Challenges:\n                {challenge_info}\n                \n                Explain the business implications of these risks and challenges. Focus on what these risk scores mean \n                for the project in practical terms. Keep the explanation clear, concise, and directly relevant to \n                business stakeholders.\n                \"\"\"\n\n            # Check if API key is available\n            if not self.groq_client.api_key:\n                self.logger.error(\"No Groq API key provided\")\n                return \"Error: No API key available. Please provide a valid Groq API key.\"\n\n            # Generate explanation using Groq\n            response = self.groq_client.chat.completions.create(\n                model=self.model_name,\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                max_tokens=500  # Increased token count for more detailed responses\n            )\n\n            return response.choices[0].message.content.strip()\n\n        except Exception as e:\n            self.logger.error(f\"Explanation generation error: {e}\")\n            return f\"Unable to generate detailed explanation. Error: {str(e)}\"\n\n    def generate_risk_chart(self, report=None, save_path=None):\n        \"\"\"\n        Generate bar chart visualization of risk factors\n        \n        Args:\n            report (dict, optional): Risk report to visualize. Uses recent report if None.\n            save_path (str, optional): Path to save the chart image, for CLI mode\n            \n        Returns:\n            matplotlib figure or None\n        \"\"\"\n        if report is None:\n            report = self.recent_report\n            \n        if not report or 'risk_names' not in report or 'risk_values' not in report:\n            return None\n            \n        try:\n            # Create figure\n            fig, ax = plt.subplots(figsize=(10, 6))\n            \n            # Get risk data\n            risk_names = report['risk_names']\n            risk_values = report['risk_values']\n            \n            # Create color map based on risk levels\n            colors = ['#91cf60' if v < 0.4 else '#fee08b' if v < 0.7 else '#fc8d59' for v in risk_values]\n            \n            # Create horizontal bar chart\n            bars = ax.barh(risk_names, risk_values, color=colors)\n            \n            # Add a vertical line at thresholds\n            ax.axvline(x=0.4, color='#fee08b', linestyle='--', alpha=0.7)\n            ax.axvline(x=0.7, color='#fc8d59', linestyle='--', alpha=0.7)\n            \n            # Add value labels to the bars\n            for bar in bars:\n                width = bar.get_width()\n                ax.text(max(width + 0.02, 0.05), \n                        bar.get_y() + bar.get_height()/2, \n                        f'{width:.2f}', \n                        va='center')\n            \n            # Set title and labels\n            ax.set_title('Project Risk Factors Analysis')\n            ax.set_xlabel('Risk Score (0-1)')\n            ax.set_xlim(0, 1.0)\n            \n            # Add legend for risk levels\n            import matplotlib.patches as mpatches\n            low_patch = mpatches.Patch(color='#91cf60', label='Low Risk (<0.4)')\n            medium_patch = mpatches.Patch(color='#fee08b', label='Medium Risk (0.4-0.7)')\n            high_patch = mpatches.Patch(color='#fc8d59', label='High Risk (>0.7)')\n            ax.legend(handles=[low_patch, medium_patch, high_patch], loc='lower right')\n            \n            # Make layout tight\n            plt.tight_layout()\n            \n            # Save the chart if path provided (for CLI mode)\n            if save_path:\n                plt.savefig(save_path)\n                plt.close(fig)\n                return save_path\n                \n            # Return figure for use in UI\n            return fig\n            \n        except Exception as e:\n            self.logger.error(f\"Chart generation error: {e}\")\n            return None\n\n    def show_explanation_window(self):\n        \"\"\"Shows a window with the detailed risk explanation\"\"\"\n        if not self.use_gui or not self.recent_report or 'explanation' not in self.recent_report:\n            return\n            \n        explanation = self.recent_report['explanation']\n        \n        # Create new window\n        explanation_window = tk.Toplevel(self.root)\n        explanation_window.title(\"Risk Explanation\")\n        explanation_window.geometry(\"600x400\")\n        \n        # Add explanation text\n        text_area = scrolledtext.ScrolledText(explanation_window, wrap=tk.WORD, font=(\"Helvetica\", 12))\n        text_area.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)\n        text_area.insert(tk.END, explanation)\n        text_area.config(state=tk.DISABLED)  # Make read-only\n\n    def show_barchart_window(self):\n        \"\"\"Shows a window with the risk barchart\"\"\"\n        if not self.use_gui or not self.recent_report:\n            return\n            \n        # Generate chart\n        fig = self.generate_risk_chart()\n        if not fig:\n            return\n            \n        # Create new window\n        chart_window = tk.Toplevel(self.root)\n        chart_window.title(\"Risk Analysis Chart\")\n        chart_window.geometry(\"800x600\")\n        \n        # Create canvas for matplotlib figure\n        canvas = FigureCanvasTkAgg(fig, master=chart_window)\n        canvas.draw()\n        canvas.get_tk_widget().pack(fill=tk.BOTH, expand=True, padx=10, pady=10)\n        \n    def start_gui(self):\n        \"\"\"Start the GUI chat interface\"\"\"\n        if not self.use_gui:\n            self.logger.warning(\"GUI mode not available, falling back to CLI\")\n            self._cli_chat()\n            return\n            \n        # Create main window\n        self.root = tk.Tk()\n        self.root.title(\"Project Risk Analysis Chatbot\")\n        self.root.geometry(\"800x600\")\n        \n        # Configure styles\n        style = ttk.Style()\n        style.configure(\"Title.TLabel\", font=(\"Helvetica\", 16, \"bold\"))\n        style.configure(\"Subtitle.TLabel\", font=(\"Helvetica\", 12))\n        style.configure(\"Risk.TLabel\", font=(\"Helvetica\", 14, \"bold\"))\n        \n        # Top frame for title\n        top_frame = ttk.Frame(self.root, padding=\"10 10 10 0\")\n        top_frame.pack(fill=tk.X)\n        \n        title_label = ttk.Label(top_frame, text=\"🚨 Project Risk Analysis Chatbot\", style=\"Title.TLabel\")\n        title_label.pack(anchor=tk.W)\n        \n        subtitle_label = ttk.Label(top_frame, \n                                   text=\"Ask about project challenges, budget variances, or specific risk concerns.\", \n                                   style=\"Subtitle.TLabel\")\n        subtitle_label.pack(anchor=tk.W, pady=(0, 10))\n        \n        # Middle frame for chat history\n        chat_frame = ttk.Frame(self.root, padding=10)\n        chat_frame.pack(fill=tk.BOTH, expand=True)\n        \n        chat_display = scrolledtext.ScrolledText(chat_frame, wrap=tk.WORD, state=tk.DISABLED)\n        chat_display.pack(fill=tk.BOTH, expand=True)\n        \n        # Bottom frame for input and buttons\n        input_frame = ttk.Frame(self.root, padding=10)\n        input_frame.pack(fill=tk.X)\n        \n        user_input = ttk.Entry(input_frame, font=(\"Helvetica\", 11))\n        user_input.pack(side=tk.LEFT, fill=tk.X, expand=True, padx=(0, 5))\n        \n        # Function to handle sending messages\n        def send_message():\n            query = user_input.get().strip()\n            if not query:\n                return\n                \n            # Display user message\n            chat_display.config(state=tk.NORMAL)\n            chat_display.insert(tk.END, \"\\n\\nYou: \" + query + \"\\n\")\n            \n            # Clear input field\n            user_input.delete(0, tk.END)\n            \n            # Check for \"explain\" command\n            if query.lower() == \"explain\":\n                if self.recent_report and 'explanation' in self.recent_report:\n                    chat_display.insert(tk.END, \"\\n💡 Detailed Explanation:\\n\")\n                    chat_display.insert(tk.END, self.recent_report['explanation'] + \"\\n\")\n                else:\n                    chat_display.insert(tk.END, \"\\nNo recent risk analysis to explain. Please ask about a project risk first.\\n\")\n                chat_display.see(tk.END)\n                chat_display.config(state=tk.DISABLED)\n                return\n                \n            # Check if query is risk-related\n            if not self.is_risk_related(query):\n                chat_display.insert(tk.END, \"\\nSorry, I can't answer that. Please ask about project risks.\\n\")\n                chat_display.see(tk.END)\n                chat_display.config(state=tk.DISABLED)\n                return\n            \n            # Process the query\n            try:\n                # Show thinking message\n                chat_display.insert(tk.END, \"\\n🤔 Analyzing risks... Please wait.\\n\")\n                chat_display.see(tk.END)\n                chat_display.update_idletasks()\n                \n                # Analyze risks\n                risk_report = self.analyze_project_risks(query)\n                \n                # Remove thinking message\n                chat_display.delete(\"end-2l\", \"end-1c\")\n                \n                # Display risk level only (no brief explanation)\n                chat_display.insert(tk.END, f\"\\n🔍 Risk Analysis Results:\\n\")\n                chat_display.insert(tk.END, f\"Risk Level: {risk_report['risk_level']} (Score: {risk_report['overall_risk']:.2f})\\n\\n\")\n                \n                # Create button frame for this response\n                chat_display.insert(tk.END, \"Type 'explain' for a detailed explanation or use these buttons:\\n\")\n                \n                # Create button for explanation\n                explanation_btn = ttk.Button(\n                    chat_display, \n                    text=\"View Detailed Explanation\", \n                    command=self.show_explanation_window\n                )\n                chat_display.window_create(tk.END, window=explanation_btn)\n                \n                # Create button for barchart\n                chat_display.insert(tk.END, \"   \")\n                barchart_btn = ttk.Button(\n                    chat_display, \n                    text=\"View Risk Chart\", \n                    command=self.show_barchart_window\n                )\n                chat_display.window_create(tk.END, window=barchart_btn)\n                \n                chat_display.insert(tk.END, \"\\n\")\n                \n            except Exception as e:\n                chat_display.insert(tk.END, f\"\\nError: {str(e)}\\n\")\n                \n            # Scroll to the end and disable editing\n            chat_display.see(tk.END)\n            chat_display.config(state=tk.DISABLED)\n        \n        # Button to send message\n        send_button = ttk.Button(input_frame, text=\"Send\", command=send_message)\n        send_button.pack(side=tk.RIGHT)\n        \n        # Bind Enter key to send message\n        self.root.bind('<Return>', lambda event: send_message())\n        \n        # Welcome message\n        chat_display.config(state=tk.NORMAL)\n        chat_display.insert(tk.END, \"🚨 Risk Analysis Chatbot: Hello! I can help you analyze project risks.\\n\")\n        chat_display.insert(tk.END, \"Ask about project challenges, budget variances, or specific risk concerns.\\n\")\n        chat_display.insert(tk.END, \"Type 'explain' after risk analysis to see detailed explanation.\\n\")\n        chat_display.config(state=tk.DISABLED)\n        \n        # Set focus to the input field\n        user_input.focus()\n        \n        # Start the GUI\n        self.root.mainloop()\n\n    def _cli_chat(self):\n        \"\"\"\n        Interactive risk analysis chat through command line\n        \"\"\"\n        print(\"🚨 Risk Analysis Chatbot: Hello! I can help you analyze project risks.\")\n        print(\"Ask about project challenges, budget variances, or specific risk concerns.\")\n        print(\"Type 'exit' to end the conversation.\")\n        print(\"Type 'explain' for the latest detailed explanation.\")\n        print(\"Type 'chart' to display risk factors (as text in CLI mode).\")\n        print(\"Type 'save_chart <filename.png>' to save the risk chart as an image.\")\n        print(\"Type 'help' to see example queries and keywords.\")\n        print(\"\\n📌 Example queries:\")\n        print(\"- What are the risks in this project?\")\n        print(\"- How can we mitigate technical issues?\")\n        print(\"- What are the challenges with AI in healthcare?\")\n        print(\"- What is the budget risk?\")\n        print(\"- Show me the chart\")\n        print(\"- Explain\")\n        print(\"\\n🔑 Keywords you can try:\")\n        print(\"  budget, timeline, resources, technical, quality, stakeholder, market, competition, delay, problem, fix\")\n    \n        while True:\n            try:\n                user_input = input(\"\\nYou: \").strip()\n    \n                if user_input.lower() in ['exit', 'quit', 'bye']:\n                    print(\"🚨 Risk Analysis Chatbot: Goodbye! Stay risk-aware.\")\n                    break\n    \n                if user_input.lower() == 'explain':\n                    if self.recent_report and 'explanation' in self.recent_report:\n                        print(\"\\n💡 Detailed Explanation:\")\n                        print(self.recent_report.get('explanation', 'No detailed explanation available.'))\n                    else:\n                        print(\"\\nNo recent risk analysis to explain. Please ask about a project risk first.\")\n                    continue\n    \n                if user_input.lower() == 'chart':\n                    if self.recent_report and 'risk_names' in self.recent_report:\n                        print(\"\\n📊 Risk Factors Chart (Text Version):\")\n                        if 'risk_names' in self.recent_report and 'risk_values' in self.recent_report:\n                            max_name_len = max(len(name) for name in self.recent_report['risk_names'])\n                            for name, value in zip(self.recent_report['risk_names'], self.recent_report['risk_values']):\n                                bar = '█' * int(value * 20)\n                                risk_level = \"HIGH\" if value >= 0.7 else \"MED\" if value >= 0.4 else \"LOW\"\n                                print(f\"{name.ljust(max_name_len)} | {bar} {value:.2f} ({risk_level})\")\n                    else:\n                        print(\"\\nNo recent risk analysis to display. Please ask about a project risk first.\")\n                    continue\n    \n                if user_input.lower().startswith('save_chart '):\n                    if self.recent_report and 'risk_names' in self.recent_report:\n                        filename = user_input[11:].strip()\n                        if not filename.endswith(('.png', '.jpg', '.pdf')):\n                            filename += '.png'\n                        try:\n                            saved_path = self.generate_risk_chart(save_path=filename)\n                            if saved_path:\n                                print(f\"Chart saved to {saved_path}\")\n                            else:\n                                print(\"Failed to save chart\")\n                        except Exception as e:\n                            print(f\"Error saving chart: {e}\")\n                    else:\n                        print(\"\\nNo recent risk analysis to save. Please ask about a project risk first.\")\n                    continue\n    \n                if not self.is_risk_related(user_input):\n                    print(\"\\nSorry, I can't answer that. Please ask about project risks.\")\n                    continue\n    \n                print(\"\\n🤔 Analyzing risks... Please wait.\")\n                risk_report = self.analyze_project_risks(user_input)\n    \n                print(f\"\\n🔍 Risk Analysis Results:\")\n                print(f\"Risk Level: {risk_report['risk_level']} (Score: {risk_report['overall_risk']:.2f})\")\n    \n                explanation = risk_report.get('explanation', '')\n                if explanation:\n                    print(\"\\n💡 Explanation:\")\n                    print(explanation)\n                else:\n                    print(\"\\n(No detailed explanation available.)\")\n    \n                print(\"\\nType 'chart' to see visual risk breakdown or 'save_chart filename.png' to export.\")\n    \n            except Exception as e:\n                print(f\"\\nError: {str(e)}\")\n\n    def chat(self):\n        \"\"\"\n        Start the chatbot interface - dispatches to CLI or GUI based on settings\n        \"\"\"\n        if self.use_gui:\n            self.start_gui()\n        else:\n            self._cli_chat()\n\nclass MarketAnalysisAgent:\n    \"\"\"Agent that analyzes market-related risks for the project.\"\"\"\n    \n    def __init__(self):\n        self.market_data = {\n            \"market_volatility\": 0.65,\n            \"financial_trends\": 0.48,\n            \"industry_competition\": 0.72\n        }\n    \n    def analyze_market_risk(self, query_text):\n        \"\"\"\n        Analyze market risks based on the query\n        \n        Args:\n            query_text (str): User's query\n            \n        Returns:\n            dict: Market risk analysis\n        \"\"\"\n        query_lower = query_text.lower()\n        \n        # Adjust baseline risk factors based on query\n        volatility = self.market_data[\"market_volatility\"]\n        financial = self.market_data[\"financial_trends\"]\n        competition = self.market_data[\"industry_competition\"]\n        \n        # Adjust based on query content\n        if \"market\" in query_lower or \"industry\" in query_lower:\n            volatility += 0.05\n            competition += 0.05\n        \n        if \"finance\" in query_lower or \"budget\" in query_lower or \"cost\" in query_lower:\n            financial += 0.1\n            \n        if \"competitor\" in query_lower or \"competition\" in query_lower:\n            competition += 0.15\n            \n        # Normalize scores to 0-1 range\n        volatility = min(max(volatility, 0), 1)\n        financial = min(max(financial, 0), 1)\n        competition = min(max(competition, 0), 1)\n        \n        # Calculate overall market risk\n        overall_risk = np.mean([volatility, financial, competition])\n        \n        # Return risk analysis\n        return {\n            \"risk_factors\": [volatility, financial, competition],\n            \"overall_market_risk\": overall_risk\n        }\n\nclass RiskScoringAgent:\n    \"\"\"Agent that scores and stores project risks.\"\"\"\n    \n    def __init__(self, collection, embedding_model):\n        self.collection = collection\n        self.embedding_model = embedding_model\n        self.risk_baseline = {\n            \"budget\": 0.7,\n            \"timeline\": 0.65,\n            \"resources\": 0.55,\n            \"technical\": 0.6,\n            \"quality\": 0.5\n        }\n    \n    def query_risk_analysis(self, query_text):\n        \"\"\"\n        Query and analyze risks based on the query text\n        \n        Args:\n            query_text (str): User's query\n            \n        Returns:\n            dict: Risk analysis results\n        \"\"\"\n        query_lower = query_text.lower()\n        \n        # Adjust baseline risk factors based on query\n        budget_risk = self.risk_baseline[\"budget\"]\n        timeline_risk = self.risk_baseline[\"timeline\"]\n        resource_risk = self.risk_baseline[\"resources\"]\n        technical_risk = self.risk_baseline[\"technical\"]\n        quality_risk = self.risk_baseline[\"quality\"]\n        \n        # Adjust based on query content\n        if \"budget\" in query_lower or \"cost\" in query_lower or \"finance\" in query_lower:\n            budget_risk += 0.1\n            \n        if \"timeline\" in query_lower or \"schedule\" in query_lower or \"deadline\" in query_lower:\n            timeline_risk += 0.1\n            \n        if \"resource\" in query_lower or \"staff\" in query_lower or \"team\" in query_lower:\n            resource_risk += 0.1\n            \n        if \"technical\" in query_lower or \"technology\" in query_lower or \"system\" in query_lower:\n            technical_risk += 0.1\n            \n        if \"quality\" in query_lower or \"defect\" in query_lower or \"bug\" in query_lower:\n            quality_risk += 0.1\n            \n        # Normalize scores to 0-1 range\n        budget_risk = min(max(budget_risk, 0), 1)\n        timeline_risk = min(max(timeline_risk, 0), 1)\n        resource_risk = min(max(resource_risk, 0), 1)\n        technical_risk = min(max(technical_risk, 0), 1)\n        quality_risk = min(max(quality_risk, 0), 1)\n        \n        # Calculate overall risk\n        overall_risk = np.mean([budget_risk, timeline_risk, resource_risk, technical_risk, quality_risk])\n        \n        # Store the query and analysis in the collection\n        if query_text:\n            embedding = self.embedding_model.encode(query_text).tolist()\n            self.collection.add(\n                embeddings=[embedding],\n                documents=[query_text],\n                metadatas=[{\n                    \"overall_risk\": float(overall_risk),\n                    \"budget_risk\": float(budget_risk),\n                    \"timeline_risk\": float(timeline_risk),\n                    \"resource_risk\": float(resource_risk),\n                    \"technical_risk\": float(technical_risk),\n                    \"quality_risk\": float(quality_risk),\n                    \"timestamp\": datetime.datetime.now().isoformat()\n                }],\n                ids=[f\"query_{datetime.datetime.now().timestamp()}\"]\n            )\n        \n        # Return risk analysis\n        return {\n            \"risk_factors\": [budget_risk, timeline_risk, resource_risk, technical_risk, quality_risk],\n            \"overall_risk\": overall_risk\n        }\n\nclass ProjectStatusTrackingAgent:\n    \"\"\"Agent that tracks project status and related risks.\"\"\"\n    \n    def __init__(self):\n        self.project_status = {\n            \"current_phase\": \"Development Phase 2\",\n            \"timeline_status\": \"Delayed\",\n            \"budget_status\": \"Over Budget\",\n            \"resource_status\": \"Understaffed\"\n        }\n    \n    def track_project_status(self):\n        \"\"\"\n        Track project status and evaluate status-related risks\n        \n        Returns:\n            dict: Project status risk analysis\n        \"\"\"\n        # Calculate risk factors based on project status\n        timeline_risk = 0.7 if self.project_status[\"timeline_status\"] == \"Delayed\" else 0.3\n        resource_risk = 0.8 if self.project_status[\"resource_status\"] == \"Understaffed\" else 0.4\n        \n        # Calculate overall status risk\n        overall_risk = np.mean([timeline_risk, resource_risk])\n        \n        # Return status risk analysis\n        return {\n            \"risk_factors\": [timeline_risk, resource_risk],\n            \"overall_status_risk\": overall_risk,\n            \"status\": self.project_status\n        }\n\nclass ReportingAgent:\n    \"\"\"Agent that generates risk reports and visualizations.\"\"\"\n    \n    def __init__(self):\n        pass\n    \n    def generate_risk_report(self, risk_data):\n        \"\"\"\n        Generate a comprehensive risk report\n        \n        Args:\n            risk_data (dict): Risk analysis data\n            \n        Returns:\n            str: Formatted risk report\n        \"\"\"\n        # Generate formatted risk report\n        report = f\"Project Risk Analysis Report\\n\"\n        report += f\"Generated on: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\"\n        \n        report += f\"Overall Risk Level: {risk_data.get('risk_level', 'Unknown')}\\n\"\n        report += f\"Overall Risk Score: {risk_data.get('overall_risk', 0):.2f}\\n\\n\"\n        \n        report += \"Risk Factors:\\n\"\n        for i, factor in enumerate(risk_data.get('risk_factors', [])):\n            factor_name = f\"Factor {i+1}\"\n            if 'risk_names' in risk_data and i < len(risk_data['risk_names']):\n                factor_name = risk_data['risk_names'][i]\n            report += f\"- {factor_name}: {factor:.2f}\\n\"\n        \n        return report\n\ndef main():\n    \"\"\"Main function to run the Project Risk Analysis Chatbot.\"\"\"\n    # Try to get API key from environment variable\n    api_key = os.environ.get(\"GROQ_API_KEY\", None)\n    \n    # Initialize and run the chatbot\n    chatbot = ProjectRiskAnalysisChatbot(\n        groq_api_key=api_key,\n        use_gui=None  # Auto-detect\n    )\n    \n    # Start chat interface\n    chatbot.chat()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T10:33:44.898210Z","iopub.execute_input":"2025-04-10T10:33:44.898554Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!du -sh /kaggle/working\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!df -h\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!free -h","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}